<!doctype html>
<html lang="en">

	<head>
		<meta charset="utf-8">

		<title>Comment tester et optimiser la performance d'un SI ?</title>

		<meta name="description" content="Comment tester et optimiser la performance d'un SI ?">
		<meta name="author" content="Cyril Picat">
		<meta name="author" content="Marc Bojoly">

		<meta name="apple-mobile-web-app-capable" content="yes" />
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/octo.css" id="theme">

		<!-- Code syntax highlighting -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>

		<!--[if lt IE 9]>
		<script src="lib/js/html5shiv.js"></script>
		<![endif]-->
	</head>
	

	<body>

		<div class="reveal">

			<!-- Any section element inside of this container is displayed as a slide -->
			<div class="slides">
				<section>
					<h2>Comment tester et optimiser la performance d'un SI ?</h2>
					<p>Softshake, 22 octobre 2015</p>
				</section>
                <section>
                    <p>
                        <h1>Marc BOJOLY</h1>
                        OCTO Technology Paris, Manager et consultant
                        Co-fondateur du Performance User Group Paris
                    </p>
                    <p>
                        <h1>Cyril PICAT</h1>
                        OCTO Technology Lausanne, consultant
                    </p>
                </section>
				<section>
					<h3>Le projet : migration d'une banque vers une nouvelle plateforme titres</h3>
					<img src="images/context/application_architecture.png" />
				</section>
				<!-- TODO: replace image instead of changing slide -->
				<section>
					<h3>Quelques chiffres</h3>
					<p>Clients : <strong>x10</strong></p>
					<p>Portefeuilles titres : <strong>x5</strong></p>
					<p>Positions : <strong>x3</strong></p>
					<p>Titres : <strong>+50%</strong></p>
					<p>Ordres de Bourse : <strong>x2</strong></p>
					<aside class="notes">
						<p>et les effets indirects : opérations sur titres etc.</p>
					</aside>
				</section>
				<section>
					<h3>Le projet : migration d'une banque vers une nouvelle plateforme titres</h3>
					<img src="images/context/application_architecture_technos.png" />
				</section>
                
				<section data-background="images/context/DSCN7614.jpg" style="background: rgba(255,255,255,0.9)">
					Autant attendre la mise en production...
					<aside class="notes">
						Lorsqu'on parle de tests de performance sur un SI il y a beaucoup d'idées reçues qui font qu'on
						attent la mise en production pour constater les problèmes
						Source : http://www.morguefile.com/archive/display/940045
					</aside>
				</section>
				<section>
					<h3>Méconnaissance face aux problèmes de performance</h3>
					<p>Ca va passer, on a bien travailler, cela va passer = nier l'exitence du problème</p>
					<p>C'est impossible à tester sauf en prod = nier l'exstence de solutions</p>
					<p>La charge ne sera pas représentative = nier la fiabilité de ces solutions</p>
					<p>On n'y arrivera jamais = mettre en doute la capacité à y arriver</p>
					<aside class="notes">
						Je rapproche ces idées reçues des 6 types de méconaissances en analyse transactionnelle
						- Nier l'existence du problème
						- Nier l'existence de solutions
						- Nier la fiabilité des solutions
						- Mettre en doute ces capacités à y arriver
						C:\Users\mbojoly\AppData\Local\Temp\_dc193685389\MyDropbox.zip\MyDropbox\CharismeDurable\Livret CD v1.pdf
					</aside>
				</section>
				<section>
					<h3>Idée reçue #1 : Le dimensionnement</h3>
					<p>La préproduction n'est pas représentative</p>
					<p>Il faudrait simuler l'ensemble des activités de la banque</p>
				</section>
				<section>
					<h3>Notre constat</h3>
					La plupart des problèmes sont des problèmes de conception
					<p>80% des problèmes ne disparaitront pas sur une machine plus grosse</p>
					<p>80% des problèmes peuvent être identifié sur un test simplifié</p>
				</section>
				<section>
					<h3>Idée reçue #2 : L'automatisation</h3>
					Vous ne pourrez pas l'automatiser, on a essayé avec <em>&lt;un nom de produit ici&gt;</em>, et personne n'y est arrivé
				</section>
				<section>
					<h3>Notre constat</h3>
					<p>Certains clients lourds sont très difficiles à tester</p>
					<p>Mais il existe des alternatives (e.g. Sikuli)</p>
					<p>Adaptez vos tests de performance en fonction des enjeux et de la complexité</p>
					<aside class="notes">
						De bons outils open source existent pour les clients légers.
						Des outils éditeurs existent pour les clients lourds (HP Load Runner, Neoload) mais je reconnais
						qu'il faut le concours de l'éditeur ou à minima les sources. Et ce n'est pas toujours possible.
						Mais il existe des alternatives comme Sikuli qui fonctionnent dans ce cas.
						Elles sont plus coûteuses à mettre en oeuvre mais c'est une alternative
					</aside>
				</section>
				<section>
					<h3>Idée reçue #3 : Les problèmes sont sur le Mainframe</h3>
				</section>
				<section>
					Pas forcément...
					<h3>Ayez systématiquement l'obsession de la mesure</h3>
					<aside class="notes">
						Le traitement d'un ordre unitaire est plus long dans le mainframe
                        Mais l'application Java batche les traitement ce qui dégrade la latence
					</aside>
				</section>
				<!-- Partie 1 -->
				<section data-background="images/fact1/StExupery.jpg" style="background: rgba(255,255,255,0.9)">
					<h3>Dans un monde parfait...</h3>
                    <blockquote>
                        &ldquo;Fais de ta vie un rêve, et d'un rêve une réalité.&rdquo;
						Antoine de Saint-Exupéry - Cahiers de Saint-Exupéry (1900-1944)
                    </blockquote>
                    <aside class="notes">
						Fait#1 : il y a le monde idéal et le monde réel
                        Je ne vais pas vous vendre du rêve, mais pour faire d'un rêve une réalité il faut quand même rêver
                        un peu. Alors dans l'idéal qu'est-ce que je voudrais
						Source des images : http://ginacn.blogspot.fr/2006/05/petites-estrelles.html
						https://en.wikipedia.org/wiki/Antoine_de_Saint-Exup%C3%A9ry
                    </aside>
				</section>
                <section>
                    <h3>Rêve 1 : Une vue intégrée de la performance</h3>
                    <img src="images/fact1/purepath1.png" height="50em"/>
                    <img src="images/fact1/purepath2.png" height="50em" />
                    <img src="images/fact1/globalview.png" height="300em" />
                    <aside class="notes">
                    	Parler d'APM sur une application, étendre multi-systèmes
                        Des outils commes les APM pour avoir une vue parfaite de l'application : comment circulent les
                        messages, où est passé le temps
                        Source Dynatrace
                    </aside>
                </section>
                <section>
                    Une parfaite connaissance de la performance : qui le fait au niveau d'un SI?
                    <img src="images/fact1/netflix.png" height="500em" />
                    <aside class="notes">
                        Sources :
                        http://www.brendangregg.com/blog/2015-06-23/netflix-instance-analysis-requirements.html
                        http://www.brendangregg.com/blog/2014-06-12/java-flame-graphs.html

                        Netflx a développé un certain nombre d'outils bas niveau qui vont produire de la donnée sur la
                        performance, mais également sur la stabilité de la plateforme et les applications
                        Et ils aggrègent tout cela.

                        Donc oui, cela existe, j'aime bien la construction de Netflix qui permet de commencer petit
                        ==> Faire la référence aux outils Grep/sed mis en place par Cyril
                    <p>Les middleware un peu anciens ou avec trop de spécificités</p>
                    <p>Les applications natives (si vous n'avez pas les sources)</p>
                                                D'où la limitation en terme d'application native : pas de possibilité d'injecter un
                        comportement au runtime

                        Aujourd'hui Dynatrace support CICS mais pas du Forms par exemple
                        Pour le code natif ils commencent à pouvoir avoir une vue système mais cela reste une bêta
                    </aside>
                </section>

				<section>
					<h3>Connaissance de la performance : par où commencer ?</h3>
                    <ul>
                        <li>Analyse de logs (python, matplotlib, python pandas...)</li>
                        <li>Collecte d'outils systèmes (nmon, vmstat...)</li>
                    </ul>
					<p>APM versus outils artisanaux</p>
				</section>
				<section>
                    <h3>Rêve 2 : On a ce qu'il faut pour faire les tests</h3>
                    <p>Des développements terminés</p>
					<p>Des données migrées</p>
                    <p>Des personnes disponibles</p>
                    <p>Et colocalisées...</p>
                </section>
                <section>
                    <h3>Les intangibles</h3>
                    <p>Un environnement opérationnel sur un jeu de données minimal</p>
                    <p>Isoler la zone de mesure</p>
                    <p>Pour le reste...</p>
                    <aside class="notes">
                        Certains éléments sont non négociables
                        Pour le reste il vous faudra être inventif (image de Mac Gyver)
                    </aside>
                </section>
				<!-- Partie 2 -->
				<section data-background="images/fact2/priorities_sign.jpg" style="background: rgba(255,255,255,0.9)">
					Savoir fixer ses priorités
				</section>
				<section>
					<div style="width: 50%; height: 100%; float: left">
						<img src="images/fact2/vertiginous3.jpg" />
					</div>
					<h3>Les problèmes peuvent sembler vertigineux</h3>
					<div class="fragment">
						<p>Préférer l'exhaustivité à la précision</p>
						<p>Ne pas chercher de solutions</p>
						<p>Timeboxer !</p>
					</div>
					<aside class="notes">
						Volume d'ordres, nombre de clients, nombre d'utilisateurs concurrents,
						capacité à comptabiliser avant la clôture comptable, durée de la nuit batch,
						impact sur les temps de réponse, latence réseau etc.
					</aside>
				</section>
				<section>
					<h3>Il faut "cadrer" le chantier</h3>
					<p>Avoir une vue précise des volumes existants et cibles</p>
					<p>Lister les problèmes existants</p>
					<p>Brainstormer sur les problèmes potentiels</p>
					<aside class="notes">
					<h3>Comment y parvenir ?</h3>
					<p>Entretiens utilisateurs</p>
					<p>Entretiens IT</p>
					</aside>
				</section>
				<section>
					<div style="width: 70%; float: left; margin-left:-100px; margin-top:50px; margin-right:10px;">
						<img width="1050px" height="600px" src="images/fact2/application_architecture_risks.png" />
					</div>
					<div style="width: 35%; float: right; font-size: 0.80em;" >
						<h3>Les problèmes "usuels"</h3>
						<ol>
							<li><strong>Capacité</strong> en terme de nombre de transactions/jour</li>
							<li>Augmentation de <strong>volumétrie</strong> (x2)</li>
							<li><strong>SLA</strong> temps de réponse end-to-end</li>
							<li>Lenteurs <strong>actuelles</strong></li>
							<li>Augmentation du <strong>nombre d'utilisateurs</strong></li>
							<li>Impact sur la <strong>durée des batchs</strong></li>
							<li><strong>Latence et temps de réponse</strong> pour les utilisateurs distants</li>
						</ol>
					</div>
					<aside class="notes">
						Capacité du HUB actuel de doubler le nombre de transactions/jour
						Capacité à gérer des OST avec 15000 clients en position
						SLA de 5 secondes sur les actions en end-to-end
						Lenteurs actuelles sur les ordres sur fonds
						Ouverture de l'application à 1000 utilisateurs front-office
						Doublement du nombre d'utilisateurs front-office
						Impact de l'augmentation du nombre de titres sur la durée des batchs
						Latence et temps de réponse pour les utilisateurs distants
					</aside>
				</section>
				<section>
					<h3>Et ensuite ?</h3>
					<p>La carte vous aide à visualiser et à prioriser, elle ne "résout" pas les problèmes</p>
					<p>Chaque problème reste complexe et lié au reste du SI</p>
					<aside class="notes">
					<h3>Construire un plan</h3>
					</aside>
				</section>
				<section>
					<h3>Diviser pour mieux régner</h3>
					<p>2 patterns</p>
					<ul>
						<li class="fragment">Diviser/découpler : passer d'un test de N systèmes à un test de k&lt;N systèmes (idéalement 1)</li>
						<br/>
						<li class="fragment">Simplifier : réduire la dimensionnalité (cas de tests, données etc.)</li>
					</ul>
				</section>
				<section>
					<h3>Diviser/découpler : un exemple</h3>
					<img height="500px" src="images/fact2/divide-and-conquer.png" />
				</section>
				<!-- TODO: replace image instead of changing slide -->
				<section>
					<h3>Diviser/découpler : un exemple</h3>
					<img height="500px" src="images/fact2/divide-and-conquer-2.png" />
					<aside class="notes">					
										<h3>Simplifier : des exemples</h3>
					<p>Ne pas tester tous les cas fonctionnels</p>
					<p>Uniquement simuler les actions principales de l'utilisateur</p>
					<p>Faire le test de charge avec des données existantes</p>
					<p>Utiliser un jeu de données restreint pour les tests</p>
					<p>! Attention, certaines simplifications seront fausses. Il faut penser à <strong>documenter les hypothèses</strong> faîtes !</p>
					</aside>
				</section>
				<section>
					<h3>Et maintenant ?</h3>
					<aside class="notes">					
					<p>Le problème est isolé et simplifié, comment je le résous ?</p>
					<p >Ne prévoyez pas un test de charge pour tous les problèmes !</p>
					</aside>
				</section>
				<section>
					<h3>Restez pragmatique !</h3>
					<ul>
						<li>Analyse de l'existant</li>
						<li>Modélisation et extrapolation</li>
						<li>Test de l'existant</li>
						<li>Test de la cible</li>
						<li>Test end-to-end intégré de la cible</li>
					</ul>
					<img class="fragment" height="250px" src="images/fact2/arrow.png" style="float:left; margin-left:150px;"/>
					<aside class="notes">					

					<br/>
					<br/>
					<p>! Certains problèmes devront passer par plusieurs étapes, afin de limiter le risque !</p>
					<p>Ne pas sautez les étapes !</p>
					</aside>
				</section>
				<section>
					<h3>Où pouvez-vous vous "planter" ?</h3>
					<div class="fragment">
						<p>D'expérience, les mauvaises décisions étaient dues à :</p>
						<ul>
							<li>une mauvaise connaissance de la performance existante</li>
							<li>une mauvaise connaissance des usages existants</li>
						</ul>
					<aside class="notes">
					<div>
						<p>Vous pouvez vous planter car :</p>
						<ul>
							<li>un risque est passé à travers le brainstorming</li>
							<li>une simplification était trop "simpliste"</li>
						</ul>
					</div>
					</aside>
					</div>
				</section>
				<!-- Partie 3 -->
                <section data-background="images/context/OBSMSR.jpg" style="background: rgba(255,255,255,0.9)">
                    <h3>Bonnes pratiques de tests de charges (unitaires)</h3>
					<aside class="notes">
                        On vous a expliquer un certain nombre de façon de faire pour les tests de performance d'un SI
                        Est-ce que cela veut dire que les techniques de tests de charge traditionnelles sont inutiles?
						http://www.morguefile.com/archive/display/907874
					<p>Les tests de charges application par application restent utiles</p>
					<p>Les bonnes pratiques qui leur sont associées également</p>
						Deux bonnes pratiques sont particulièrement importantes pour des tests de performance d'un SI
					</aside>
                </section>
                <section>
                    <h3>Délimiter le périmètre testé</h3>
                    <p>Car un test de charge reste un test automatisé</p>
                    <p>Car un test en erreur c'est un test qui ne sert à rien</p>
                    <aside class="notes">Si vous ne délimitez pas correctement le périmètre testé vous allez au devant de
                    problèmes insurmontables.
                    </aside>
                    <h3>Comment le délimiter ?</h3>
                    <p>Choisissez soigneusement votre jeux de données</p>
                    <p>Ou développez des bouchons</p>
                    <aside class="notes">
                        Définissez soigneusement votre jeu de données de façon à ce que l'ensemble des systèmes sollicités
                        répondent correctement. Etudiez la charge que cela va induire dans tous les systèmes avals :
                        requête sur une seule et même resssource ? requêtes sur un jeu de données élargi ? sur un jeu
                        de données représentatif ?
                    </aside>
                </section>
                <section>
                    <h1>Mettre un exemple concret ?</h1>
                    TODO : GED Cardiff
                </section>
                <section>
                    <h2>Modélisation de la charge</h2>
					<p>Comment modéliser le comportement de mes utilisateurs ?</p>
					<p>Combien d'utilisateurs simultanés ?</p>
					<p>Qu'est-ce qu'un utilisateur simultané ?</p>
                    <aside class="notes">
                        Pour une application existante connaître le nombre d'utilisateurs est assez simple.
                        Mais pour un SI entier ? Pour une application qui ne reçoit des demandes que d'autres
                        applications ?
                    </aside>
                </section>
				<section>
					Modélisation de la charge : Comment modéliser le comportement de mes utilisateurs ?
					<p>La véritée est en production</p>
					<img height="238" data-src="./images/mdlcharge/stats.png" alt="Modèles statistiques">
					<p>Construisez un modèle à partir de vos statistiques de production</p>
					<aside class="notes">
						La mesure d'un temps de réponse sera toujours statistique. En effet l'arrivée des utilsiateurs sera toujours aléatoire.
						Tous les tests de charge vont donc reposer sur la mesure statistique du temps de réponse suite à une injection. 
						Le modélisation des uti
                        lisateurs est basée sur une statistique d'arrivée et une statistique d'un temps de réflexion (expliquer)
						<a href="F:\users\mbojoly\Google Drive\Pole Performance\Chantiers\OctoAcademy\Performance et scalabilite.pptx">Image</a>
					</aside>
				</section>
				<section>
					<p>Modélisation de la charge : Soyez prédictif</p>
					<p>Si vous n'avez pas de production, proposez un modèle en fonction des informations que vous avez.</p>
					<img height="238" data-src="./images/mdlcharge/adopters.png" alt="Modèle de prévision">
					<aside class="notes">
					Par exemple basez vous sur des modèles métiers : combien de nouveaux utilisateurs ?
					</aside>
				</section>
				<section>
					<p>Modélisation de la charge : Soyez prédictif</p>
					<p>Si vous n'avez pas de production, proposez un modèle en fonction des informations que vous avez.</p>
					<img height="238" data-src="./images/mdlcharge/trafficInternet.png" alt="Modèle de prévision">
					<aside class=   "notes">
					Par exemple basez vous sur des modèles métiers : quelle est l'utilisation classique d'internet ?
					<a href="http://www.libstat.com/pages/heure.htm">Libstat</a> 
					<a href="F:\Users\mbojoly\Documents\MBO\2014.zip\2014\MyDrive\R2014-168 PARKEON - Tests de perfs Plateforme billettique Lille\4 - Documents de travail\ModeleMonteeEnCharge.xlsx">Image</a>
					</aside>
				</section>
				<section>
					<p>Modélisation de la charge : comment l'utiliser ?</p>
					Définissez dans vos tests d'injection locaux le nombre d'utilisateurs "simultanés" et le temps de réflexion
					<pre><code data-trim contenteditable class="scala" >
val clientSearchChain = group("client_search_page") {
	exec(http("client_search_html")
	.get("""/ebankingAdmin/acibf/root/contract/contractlist/""")
}
).pause(7,8) //Pause between 7 and 8 seconds

val scn = scenario("AdminSimulation")
	.repeat(1) { //Beware of the coordinated omission
		exitBlockOnFail {
			exec(loginChain)
			.exec(clientSearchChain)
            //No logout, 90% of users don't
		}
	}

setUp(scn.inject(
    rampUsers(120).over(60), //Needs to be adapted to your scenario
).protocols(httpProtocol)
</code></pre>
					<aside class="notes">
                        Coordinated Omission, c'est quant ton injecteur réduit sa charge proportionnellement à
                        la lenteur de ton applicatif en charge. Ça arrive avec LoadRunner parce qu'il compte en
                        utilisateurs simultanés et pas en nouveaux utilisateurs par unité de temps ou actions par unité de temps.
                        <a href="F:\users\mbojoly\Google Drive\SHARED ASSETS\SHARED ASSETS - PERF\06. PERF - Advanced users\Formations\Pepperdine_20140707\performance.zip\performance\slides\04_benchmarking.pdf">
                            Slide 86 Formation Kirk</a>
					</aside>
				</section>
                <section>
                    <h3>Ne négligez pas les tests de charge unitaire</h3>
                    <p><span>Comment avaler un éléphant ?</span>&nbsp;<span>Bouchée par bouchée</span></p>
                    <p></p>
                    <aside class="notes">
                        Ne négligez pas les tests de charge unitaire. Choisissez les le plus précisément possible pour
                        qu'ils soient simples, reproductibles, compréhensibles.
                        C'est le seul moyen de réutiliser ces résultats dans des conclusions plus larges
                    </aside>
                </section>
				<!-- Partie 4 -->
				<section data-background="images/fact4/tests-end-to-end.jpg" style="background: rgba(255,255,255,0.9)">
					Comment (quand même) tester en end-to-end ?
				</section>
                <section>
                    <section>
                        <h3>DEMO : une application</h3>
                        <p>5 appels en base de données, 1 traitement</p>
                    </section>
                    <section>
					<pre><code data-trim contenteditable class="shell" >
                        curl -X POST \
                        -H "Accept: applicaiton/json" \
                        -H "Content-Type: application/json" \
                        -d '{"cpuIntensiveComputationsDuration":50, "databaseCallsNumber":5, "databaseCallDuration":10 }' \
                        http://192.168.99.100:8080/compute
                    </code></pre>
                    </section>
                </section>
               <section>
					<section>
						<h3>DEMO Quizz : quel est le temps de réponse d'une application ?</h3>
                        <p>1 application</p>
                        <p>60 ms. de traitement</p>
						<p>6 appels en base de données, 10 ms. chacuns</p>
					</section>
                    <section><pre><code data-trim contenteditable class="shell" >
                        curl -X POST \
                        -H "Accept: applicaiton/json" \
                        -H "Content-Type: application/json" \
                        -d '{"cpuIntensiveComputationsDuration":60, "databaseCallsNumber":6, "databaseCallDuration":10 }' \
                        http://$HOST:8080/compute
						</code></pre>
                    </section>
                   <section><code>
                       <pre>$ ./sh/poc1.sh
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100   234  100   142  100    92    533    345 --:--:-- --:--:-- --:--:--   533Call the database 6 times during 10 ms. each for a total of 198 ms.
CPU intensive compute 49ms.
Call HTTP Ressources : For a total of 0.0 ms.
</pre>
                   </code></section>
                </section>
				<section>
					<section>
						<h3>DEMO & Quizz : quel est le temps de réponse d'une chaîne applicative dans un SI ?</h3>
                        <p>6 applications identiques à la précédente (60 ms. de traitement, 6 x 10 ms. de BD)</p>
                        <p>Appels synchrones séquentiels</p>
					</section>
                    <section><pre><code data-trim contenteditable class="shell" >
                        curl -X POST \
                        -H "Accept: applicaiton/json" \
                        -H "Content-Type: application/json" \
                        -d '{"cpuIntensiveComputationsDuration":60, "databaseCallsNumber":6, "databaseCallDuration":12, "serviceCalls":[{"computationDescription":{"cpuIntensiveComputationsDuration":60, "databaseCallsNumber":6, "databaseCallDuration":5}, "callsNumber":2 }, {"computationDescription":{"cpuIntensiveComputationsDuration":60, "databaseCallsNumber":6, "databaseCallDuration":5}, "callsNumber":2 }, {"computationDescription":{"cpuIntensiveComputationsDuration":60, "databaseCallsNumber":6, "databaseCallDuration":5}, "callsNumber":2 }, {"computationDescription":{"cpuIntensiveComputationsDuration":60, "databaseCallsNumber":6, "databaseCallDuration":5}, "callsNumber":2 }, {"computationDescription":{"cpuIntensiveComputationsDuration":60, "databaseCallsNumber":6, "databaseCallDuration":5}, "callsNumber":2 }]}' \
                        http://$HOST:8080/compute
                    </code></pre></section>
                </section>
                <section><code>
                    <pre>$ ./sh/poc2.sh
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  2380  100  1587  100   793   1005    502  0:00:01  0:00:01 --:--:--  1005Call the database 6 times during 12 ms. each for a total of 100 ms.
CPU intensive compute 56ms.
Call HTTP Ressources :{
Call the database 6 times during 5 ms. each for a total of 57 ms.
CPU intensive compute 44ms.
Call HTTP Ressources : For a total of 0.0 ms.}
{
Call the database 6 times during 5 ms. each for a total of 50 ms.
CPU intensive compute 48ms.
Call HTTP Ressources : For a total of 0.0 ms.}
{
Call the database 6 times during 5 ms. each for a total of 156 ms.
CPU intensive compute 51ms.
Call HTTP Ressources : For a total of 0.0 ms.}
{
Call the database 6 times during 5 ms. each for a total of 49 ms.
CPU intensive compute 58ms.
Call HTTP Ressources : For a total of 0.0 ms.}
{
Call the database 6 times during 5 ms. each for a total of 122 ms.
CPU intensive compute 58ms.
Call HTTP Ressources : For a total of 0.0 ms.}
{
Call the database 6 times during 5 ms. each for a total of 44 ms.
CPU intensive compute 50ms.
Call HTTP Ressources : For a total of 0.0 ms.}
{
Call the database 6 times during 5 ms. each for a total of 47 ms.
CPU intensive compute 59ms.
Call HTTP Ressources : For a total of 0.0 ms.}
{
Call the database 6 times during 5 ms. each for a total of 46 ms.
CPU intensive compute 49ms.
Call HTTP Ressources : For a total of 0.0 ms.}
{
Call the database 6 times during 5 ms. each for a total of 49 ms.
CPU intensive compute 58ms.
Call HTTP Ressources : For a total of 0.0 ms.}
{
Call the database 6 times during 5 ms. each for a total of 45 ms.
CPU intensive compute 51ms.
Call HTTP Ressources : For a total of 0.0 ms.}
 For a total of 1418.0 ms.
</pre>
                </code></section>
                <section>
                    <section>
                        <h3>DEMO & Quizz : quel est le temps de réponse de la même chaîne dans une modélisation plus proche de la réalité ?</h3>
                        <p>6 applications identiques à la précédente (60 ms. de traitement, 6 x 10 ms. de BD)</p>
                        <p>Appels synchrones séquentiels</p>
                    </section>
					<section><pre><code data-trim contenteditable class="shell" >
$ sudo docker-machine ssh default
$$ sudo tc qdisc add dev docker0 root netem delay 10ms
$$ sudo tc qdisc show dev docker0
					</code></pre></section>
                    <section><code>
                        <pre>$ ./sh/poc2.sh
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  2388  100  1595  100   793    398    198  0:00:04  0:00:04 --:--:--   398Call the database 6 times during 12 ms. each for a total of 333 ms.
CPU intensive compute 57ms.
Call HTTP Ressources :{
Call the database 6 times during 5 ms. each for a total of 295 ms.
CPU intensive compute 58ms.
Call HTTP Ressources : For a total of 0.0 ms.}
{
Call the database 6 times during 5 ms. each for a total of 291 ms.
CPU intensive compute 58ms.
Call HTTP Ressources : For a total of 0.0 ms.}
{
Call the database 6 times during 5 ms. each for a total of 293 ms.
CPU intensive compute 58ms.
Call HTTP Ressources : For a total of 0.0 ms.}
{
Call the database 6 times during 5 ms. each for a total of 303 ms.
CPU intensive compute 58ms.
Call HTTP Ressources : For a total of 0.0 ms.}
{
Call the database 6 times during 5 ms. each for a total of 294 ms.
CPU intensive compute 57ms.
Call HTTP Ressources : For a total of 0.0 ms.}
{
Call the database 6 times during 5 ms. each for a total of 291 ms.
CPU intensive compute 58ms.
Call HTTP Ressources : For a total of 0.0 ms.}
{
Call the database 6 times during 5 ms. each for a total of 291 ms.
CPU intensive compute 55ms.
Call HTTP Ressources : For a total of 0.0 ms.}
{
Call the database 6 times during 5 ms. each for a total of 293 ms.
CPU intensive compute 58ms.
Call HTTP Ressources : For a total of 0.0 ms.}
{
Call the database 6 times during 5 ms. each for a total of 294 ms.
CPU intensive compute 58ms.
Call HTTP Ressources : For a total of 0.0 ms.}
{
Call the database 6 times during 5 ms. each for a total of 289 ms.
CPU intensive compute 58ms.
Call HTTP Ressources : For a total of 0.0 ms.}
 For a total of 3575.0 ms.
</pre>
                    </code></section>
                    <section>
                        <h3>POC & Quizz</h3>
                        <img src="images/fact4/PocSchema.png" />
                        <aside class="notes">
                            L'objectif de ce POC est de modéliser un SI pour voir l'impact de ces problèmes
                            Pour cela nous avons utilisé docker pour représenter les applications au sein d'une seule VM
                        </aside>
                    </section>
				</section>
                <section><img src="images/fact4/Peage.jpg"></section>
                <section>
                    <h3>Nous avons simulé une DMZ un peu pointilleuse</h3>
                    <img src="images/fact4/DMZ.png" />
                </section>
                <section>
                    <h3>Et concrêtement ? </h3>
                    <img src="images/fact4/DockerTcp80.png" />
                </section>
                <section>
                    <h3>Et concrêtement ?</h3>
                    <img src="images/fact4/DockerTcp9093.png" />
                </section>
                <section>
                    <h3>En synthèse, de faibles problèmes peuvent devenir critiques au niveau d'un SI</h3>
                    <ul>
                        <li>Latence</li>
                        <li>N+1 requêtes</li>
                        <li>N+1 applications</li>
                    </ul>
                </section>
				<section>
					<p>Les tests end-to-end sont obligatoires car certains résultats (cf. PoC) peuvent défier l'intuition</p>
					<div class="fragment">
						<h3 >Disclaimer</h3>
						<p>Les tests end-to-end sont complexes. Ceci ne se veut pas une méthode systémique mais un inventaire
							d'outils à disposition, ainsi qu'un REX sur leur efficacité</p>
					</div>
				</section>
				<section>
					<h3>Outil #1 : Analyse de la production existante</h3>
					<p>Analyse des latences entre systèmes</p>
					<p>Identification du goulet d'étranglement</p>
					<p>Découverte des problèmes de design</p>
					<p>Évaluation de la capacité du système</p>
				</section>
				<section>
					<h3>Outil #1 : Analyse de la production existante</h3>
					<p>Exemple de l'évaluation de la capacité du système</p>
					<img src="images/fact4/order-throughput.png" />
					<aside class="notes">
					<h3>Outil #1 : mon REX</h3>
					<p>Souvent faîte trop tard...</p>
					<p>Commencez petit...</p>
					<p>Attention à la répartition de la charge !</p>
					</aside>
				</section>
				<section>
					<h3>Outil #1 : attention aux chiffres !</h3>
					<div class="fragment">
					<p>Jour 1 : 1000 transactions, temps de réponse moyen 15s</p>
					<p>Jour 2 : 2000 transactions, temps de réponse moyen 40s</p>
					</div>
				<aside class="notes">
					Loi de scalabilité universelle
					Profil de la journée similaire
					Détail
				</aside>					
					<!-- TODO add image comparison -->
				</section>
				<section>
					<h3>Outil #2 : Benchmark unitaire</h3>
					<p><em>Définition : mesure de la réponse à une transaction unitaire</em></p>
				</section>
				<section>
					<h3>Outil #2 : Benchmark unitaire</h3>
					<p>Exemple d'analyse</p>
					<img height="200px" src="images/fact4/response-time.png" />
					<div class="fragment">
						<p>Une représentation en "flame graph" est également adaptée</p>
						<img height="150px" src="images/fact4/flame-graph.png" />
					</div>
				<aside class="notes">
					<h3>Outil #2 : mon REX</h3>
					<p>La plupart des problèmes de design apparaissent en unitaire</p>
					<p>Unitaire vs unique</p>
					<p>S'il ne faut en garder qu'un, gardez celui-ci !</p>
				</aside>
				</section>
				<section>
					<h3>Outil #3 : le pic de transactions</h3>
					<img height="400px" src="images/fact4/dirac.png" />
					<aside class="notes">
						Peak de N transactions
						ou 1 transaction sur un gros volume
					</aside>
				</section>
				<section>
					<h3>Outil #3 : le pic de transactions</h3>
					<p>Exemple d'analyse</p>
					<img height="400px" src="images/fact4/dirac-example.png" />
				<aside class="notes">
					<h3>Outil #3 : mon REX</h3>
					<p>Le complément parfait du test unitaire</p>
					<p>Plus simple à simuler qu'une charge réaliste</p>
					<p>Pas toujours pertinent : ex. utilisateurs</p>
					<p>Teste également la robustesse du système</p>
					<p>Greffez-vous sur les tests fonctionnels !</p>
					</aside>
				</section>
				<section>
					<h3>Outil #4 : "rejeu" d'une journée de production</h3>
					<img height="500px" src="images/fact4/replay-production.png" />
					<p class="fragment" style="position: absolute; top:450px; left: 200px; height:50px; width:600px; font-size:1.5em; color:blue;">simplifiée bien sûr !</p>
				<aside class="notes">
					<h3>Outil #4 : mon REX</h3>
					<p>Difficile à analyser, notamment les écarts</p>
					<p>Difficile à interpréter</p>
					<p>En dernier, en impliquant le support métier</p>
				</aside>
				</section>
				<section>
					<h3>Outil #5 : mettre en production</h3>
					<p>Tout ce qui peut être mis en production par avance doit l'être</p>
					<p>Pensez votre stratégie de migration pour monter en charge progressivement</p>
					<p>Inspirez vous des Géants du Web</p>
				<aside class="notes">
					 (feature flipping, dark launch, canary release etc.)
				</aside>
				</section>
				<section>
					<h3>Que faut-il mesurer ?</h3>
					<!-- TODO -->
					<p>Temps entrée / sortie de chaque système</p>
					<p class="fragment">à complèter si nécessaire...</p>
				</section>
				<!-- Conclusion -->
				<section>
					<h3>Conclusion</h3>
					<ul>
						<li>Faites des tests, même imparfaits</li>
						<li>Mesurez scientifiquement</li>
						<li>Revenez-en à des problèmes simples</li>
						<li>Extrapolez, en ayant conscience des limites</li>
					</ul>
                    <aside class="notes">
					<p>Conclusion généralisation capacité vs temps de réponse</p>
					<h3>Peut-on extrapoler (à la production) ?</h3>
					<p>Beaucoup d'extrapolations différentes possibles</p>
					<p>Environnement, concurrence, simplifications...</p>
					<p>Mon expérience : OUI (sauf le sizing machine)</p>
					<p>mais attention de rester "linéaire" dans vos tests !</p>
					</aside>					
				</section>
                <section data-background="images/context/DSCN7614.jpg" style="background: rgba(255,255,255,0.9)">
                    	"Tous les modèles sont faux, certains sont utiles"
                    <aside class="notes">
                        Désormais vous êtes beaucoup mieux outillés pour gérer les problèmes de performance
                        en production

                    </aside>
                </section>


			</div>

		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>

			// Full list of configuration options available at:
			// https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({
				controls: true,
				progress: true,
				history: true,
				center: true,
				slideNumber: true,

				transition: 'slide', // none/fade/slide/convex/concave/zoom

				// Optional reveal.js plugins
				dependencies: [
					{ src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
					{ src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/highlight/highlight.js', async: true, condition: function() { return !!document.querySelector( 'pre code' ); }, callback: function() { hljs.initHighlightingOnLoad(); } },
					{ src: 'plugin/zoom-js/zoom.js', async: true },
					{ src: 'plugin/notes/notes.js', async: true }
				]
			});

		</script>

	</body>
</html>
