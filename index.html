<!doctype html>
<html lang="en">

	<head>
		<meta charset="utf-8">

		<title>Softshake : Performance du SI</title>

		<meta name="description" content="Performance du SI">
		<meta name="author" content="Cyril Picat">
		<meta name="author" content="Marc Bojoly">

		<meta name="apple-mobile-web-app-capable" content="yes" />
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/octo.css" id="theme">

		<!-- Code syntax highlighting -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>

		<!-- override default black borders for images -->
		<style>
		
			.reveal section img {
				border: none;
		  		box-shadow: 0 0 0 rgba(0, 0, 0, 0); 
			}

		</style>

		<!--[if lt IE 9]>
		<script src="lib/js/html5shiv.js"></script>
		<![endif]-->
	</head>
	

	<body>

		<div class="reveal">

			<!-- Any section element inside of this container is displayed as a slide -->
			<div class="slides">
				<section>
					<h1>Performance du SI</h1>
				</section>
                <section>
                    <p>
                        <h1>Marc BOJOLY</h1>
                        OCTO Technology Paris, Manager et consultant
                        Co-fondateur du Performance User Group Paris
                    </p>
                    <p>
                        <h1>Cyril PICAT</h1>
                        OCTO Technology Lausanne, consultant
                    </p>
                </section>
				<section data-background="images/context/DSCN7614.jpg" style="background: rgba(255,255,255,0.9)">
					Autant attendre la mise en production...
					<aside class="notes">
						Lorsqu'on parle de tests de performance sur un SI il y a beaucoup d'idées reçues qui font qu'on
						attent la mise en production pour constater les problèmes
						Source : http://www.morguefile.com/archive/display/940045
					</aside>
				</section>
				<section>
					<h3>Le projet : migration d'une banque vers une nouvelle plateforme titres</h3>
					<img src="images/context/application_architecture.png" />
				</section>
				<!-- TODO: replace image instead of changing slide -->
				<section>
					<h3>Le projet : migration d'une banque vers une nouvelle plateforme titres</h3>
					<img src="images/context/application_architecture_technos.png" />
				</section>
				<section>
					<h3>Quelques chiffres</h3>
					<p class="fragment">Clients : <strong>x10</strong></p>
					<p class="fragment">Portefeuilles titres : <strong>x5</strong></p>
					<p class="fragment">Positions : <strong>x3</strong></p>
					<p class="fragment">Titres : <strong>+50%</strong></p>
					<p class="fragment">Ordres de Bourse : <strong>x2</strong></p>
					<p class="fragment">et les effets indirects : opérations sur titres etc.</p>
				</section>
				<section>
					<h3>Idées reçues</h3>
					<p class="fragment">Ca va passer, on a bien travailler, cela va passer = nier l'exitence du problème</p>
					<p class="fragment">C'est impossible à tester sauf en prod = nier l'exstence de solutions</p>
					<p class="fragment">La charge ne sera pas représentative = nier la fiabilité de ces solutions</p>
					<p class="fragment">On n'y arrivera jamais = mettre en doute la capacité à y arriver</p>
					<aside class="notes">
						Je rapproche ces idées reçues des 6 types de méconaissances en analyse transactionnelle
						- Nier l'existence du problème
						- Nier l'existence de solutions
						- Nier la fiabilité des solutions
						- Mettre en doute ces capacités à y arriver
						C:\Users\mbojoly\AppData\Local\Temp\_dc193685389\MyDropbox.zip\MyDropbox\CharismeDurable\Livret CD v1.pdf
					</aside>
				</section>
				<section>
					<h3>Ma promesse à la fin de ces quelques slides</h3>
					<p class="fragment">Vous convaincre qu'une approche est possible</p>
					<p class=fragment">Et vous aider à combattre les idées reçues !</p>
				</section>
				<section>
					<h3>Idée reçue#1 : Le dimensionnement</h3>
					Idées reçues :
					<p class="fragment">La préproduction n'est pas représentative</p>
					<p class="fragment">Il faudrait simuler l'ensemble des actvités de la banque</p>
				</section>
				<section>
					<h3>Notre constat</h3>
					La plupart des problèmes sont des problèmes de conception
					<p class="fragment">80% des problèmes peuvent déjà être identifiés sans dimensionnement correct</p>
					<p class="fragment">100% des problèmes avec une petite machine seront présents sur une machine plus grosse</p>
				</section>
				<section>
					<h3>Idée reçue#2 : L'automatisation</h3>
					Vous ne pourrez pas l'automatiser on l'a essayé avec QTP et personne n'y est arrivé
				</section>
				<section>
					<h3>Notre constat</h3>
					<p class="fragment">Certains clients lourds sont très difficiles à tester</p>
					<p class="fragment">Mais il existe des alternatives (e.g. Sikuli)</p>
					<p class="fragment">Adapter vos tests de performance en fonction des enjeux et de la complexité</p>
					<aside class="notes">
						De bons outils open source existent pour les clients légers.
						Des outils éditeurs existent pour les clients lourds (HP Load Runner, Neoload) mais je reconnais
						qu'il faut le concours de l'éditeur ou à minima les sources. Et ce n'est pas toujours possible.
						Mais il existe des alternatives comme Sikuli qui fonctionnent dans ce cas.
						Elles sont plus coûteuses à mettre en oeuvre mais c'est une alternative
					</aside>
				</section>
				<section>
					<h3>Idée reçue#3 : Les problèmes sont sur le mainframe</h3>
				</section>
				<section data-background="images/context/OBSMSR.jpg" style="background: rgba(255,255,255,0.9)">
					Pas forcément...
					<h3>Ayez systématiquemnent l'obsession de la mesure</h3>
					<aside class="notes">Dans notre cas la lenteur principale était sur l'application Java</aside>
				</section>
				<!-- Partie 1 -->
				<section data-background="images/fact1/StExupery.jpg" style="background: rgba(255,255,255,0.9)">
					<h3>Dans un monde parfait...</h3>
                    <blockquote>
                        &ldquo;Fais de ta vie un rêve, et d'un rêve une réalité.&rdquo;
						Antoine de Saint-Exupéry - Cahiers de Saint-Exupéry (1900-1944)
                    </blockquote>
                    <aside class="notes">
						Fait#1 : il y a le monde idéal et le monde réel
                        Je ne vais pas vous vendre du rêve, mais pour faire d'un rêve une réalité il faut quand même rêver
                        un peu. Alors dans l'idéal qu'est-ce que je voudrais
						Source des images : http://ginacn.blogspot.fr/2006/05/petites-estrelles.html
						https://en.wikipedia.org/wiki/Antoine_de_Saint-Exup%C3%A9ry
                    </aside>
				</section>
                <section>
                    <h3>Rêve 1 : Avoir une parfaite connaissance de la performance</h3>
                    <img src="images/fact1/purepath1.png" height="50em"/>
                    <img src="images/fact1/purepath2.png" height="50em" />
                    <img src="images/fact1/globalview.png" height="300em" />
                    <aside class="notes">
                        Des outils commes les APM pour avoir une vue parfaite de l'application : comment circulent les
                        messages, où est passé le temps
                        Source Dynatrace
                    </aside>
                </section>
                <section>
                    <h3>APM : les limites</h3>
                    <p class="fragment">Les middleware un peu anciens ou avec trop de spécificités</p>
                    <p class="fragment">Les applications natives (si vous n'avez pas les sources)</p>
                    <aside class="notes">
                        Les APM : pour Application Performance Management vont peu ou prou donner une vision globale
                        en injectant un identifiant dans chaque requête qui est ensuite diffusé à travers tout
                        mécanisme que l'outil connaît.
                        Pour donner la vision dans l'application ils vont réaliser de l'injection de bytecode.

                        D'où la limitation en terme d'application native : pas de possibilité d'injecter un
                        comportement au runtime

                        Aujourd'hui Dynatrace support CICS mais pas du Forms par exemple
                        Pour le code natif ils commencent à pouvoir avoir une vue système mais cela reste une bêta
                    </aside>
                </section>
                <section>
                    Une parfaite connaissance de la performance : qui le fait au niveau d'un SI?
                    <img src="images/fact1/netflix.png" height="500em" />
                    <aside class="notes">
                        Source : http://www.brendangregg.com/blog/2015-06-23/netflix-instance-analysis-requirements.html
                        Netflx a développé un certain nombre d'outils bas niveau qui vont produire de la donnée sur la
                        performance, mais également sur la stabilité de la plateforme et les applications
                        Et ils aggrègent tout cela.

                        Donc oui, cela existe, j'aime bien la construction de Netflix qui permet de commencer petit
                        ==> Faire la référence aux outils Grep/sed mis en place par Cyril
                    </aside>
                </section>

				<section>
					<h3>Quel outillage ?</h3>
					<!-- TODO -->
					<p class="fragment">APM versus outils artisanaux</p>
				</section>
				<section>
                    <h3>Rêve 2 : On a ce qu'il faut pour faire les tests</h3>
                    <p class="fragment">Des développements terminés</p>
					<p class="fragment">Des données migrées</p>
                    <p class="fragment">Des personnes disponibles</p>
                    <p class="fragment">Et colocalisées...</p>
                </section>
                <section>
                    <h3>Les intangibles</h3>
                    <p>Un environnement opérationnel sur un jeu de données minimal</p>
                    <p>Isoler la zone de mesure</p>
                    <p>Pour le reste...</p>
                    <aside class="notes">
                        Certains éléments sont non négociables
                        Pour le reste il vous faudra être inventif (image de Mac Gyver)
                    </aside>
                </section>
				<!-- Partie 2 -->
				<section data-background="images/fact2/priorities_sign.jpg" style="background: rgba(255,255,255,0.9)">
					Savoir fixer ses priorités
				</section>
				<section>
					<h3>Il faut "cadrer" le chantier</h3>
					<p class="fragment">Avoir une vue précise des volumes existants et cibles</p>
					<p class="fragment">Lister les problèmes existants</p>
					<p class="fragment">Brainstormer sur les problèmes potentiels</p>
				</section>
				<!-- TODO: replace paragraph instead of changing slide -->
				<section>
					<div style="width: 50%; height: 100%; float: left">
						<img src="images/fact2/vertiginous3.jpg" />
					</div>
					<h3>Les problèmes peuvent sembler vertigineux</h3>
					<p class="fragment">Ne pas se perdre dans les détails, ne pas aborder les solutions</p>
					<p class="fragment">Préférer l'exhaustivité à la précision</p>
					<p class="fragment">Timeboxer !</p>
					<aside class="notes">
						Volume d'ordres, nombre de clients, nombre d'utilisateurs concurrents,
						capacité à comptabiliser avant la clôture comptable, durée de la nuit batch,
						impact sur les temps de réponse, latence réseau etc.
					</aside>
				</section>
				<section>
					<h3>Comment y parvenir ?</h3>
					<p class="fragment">Entretiens utilisateurs</p>
					<p class="fragment">Entretiens IT</p>
					<p class="fragment">Problèmes existants vs problèmes potentiels</p>
				</section>
				<section>
					<div style="width: 60%; float: left; margin-left:-40px; margin-top:120px; margin-right:20px;">
						<img src="images/fact2/application_architecture_risks.png" />
					</div>
					<div style="width: 40%; float: right; font-size: 0.80em;" >
						<h3>Les problèmes "usuels"</h3>
						<ol>
							<li><strong>Capacité</strong> en terme de nombre de transactions/jour</li>
							<li>Augmentation de <strong>volumétrie</strong> (x2)</li>
							<li><strong>SLA</strong> temps de réponse end-to-end</li>
							<li>Lenteurs <strong>actuelles</strong></li>
							<li>Augmentation du <strong>nombre d'utilisateurs</strong></li>
							<li>Impact sur la <strong>durée des batchs</strong></li>
							<li><strong>Latence et temps de réponse</strong> pour les utilisateurs distants</li>
						</ol>
					</div>
					<aside class="notes">
						Capacité du HUB actuel de doubler le nombre de transactions/jour
						Capacité à gérer des OST avec 15000 clients en position
						SLA de 5 secondes sur les actions en end-to-end
						Lenteurs actuelles sur les ordres sur fonds
						Ouverture de l'application à 1000 utilisateurs front-office
						Doublement du nombre d'utilisateurs front-office
						Impact de l'augmentation du nombre de titres sur la durée des batchs
						Latence et temps de réponse pour les utilisateurs distants
					</aside>
				</section>
				<section>
					<h3>Et ensuite ?</h3>
					<h3>Construire un plan</h3>
					<p class="fragment">La carte vous aide à visualiser et à prioriser, elle ne "résout" pas les problèmes</p>
					<p class="fragment">La réponse à chaque problème pourra (devra) être complètement différente !</p>
					<p class="fragment">Chaque problème reste complexe et lié au reste du SI</p>
				</section>
				<section>
					<h3>Diviser pour mieux régner</h3>
					<p>2 patterns</p>
					<ul>
						<li class="fragment">Diviser/découpler : passer d'un test de N systèmes à un test de k&lt;N systèmes (idéalement 1)</li>
						<li class="fragment">Simplifier : réduire la dimensionnalité (cas de tests, données)</li>
					</ul>
				</section>
				<section>
					<h3>Diviser/découpler : un exemple</h3>
					<img src="images/fact2/divide-and-conquer.png" />
				</section>
				<!-- TODO: replace image instead of changing slide -->
				<section>
					<h3>Diviser/découpler : un exemple</h3>
					<img src="images/fact2/divide-and-conquer-2.png" />
				</section>
				<section>
					<h3>Diviser/découpler : généralisons !</h3>
					<img height="500px" src="images/fact2/divide-and-conquer-3.png" />
				</section>
				<!-- TODO: replace image instead of changing slide -->
				<section>
					<h3>Diviser/découpler : généralisons !</h3>
					<img height="500px" src="images/fact2/divide-and-conquer-4.png" />
				</section>
				<section>
					<h3>Simplifier : des exemples</h3>
					<p class="fragment">Ne pas tester tous les cas fonctionnels</p>
					<p class="fragment">Uniquement simuler les actions principales de l'utilisateur</p>
					<p class="fragment">Faire le test de charge avec des données existantes</p>
					<p class="fragment">Utiliser un jeu de données restreint pour les tests</p>
					<p class="fragment">! Attention, certaines simplifications seront fausses. Il faut penser à <strong>documenter les hypothèses</strong> faîtes !</p>
				</section>
				<section>
					<h3>Et maintenant ?</h3>
					<p>Le problème est isolé et simplifié, comment je le résous ?</p>
					<p  class="fragment">Ne prévoyez pas un test de charge pour tous les problèmes !</p>
				</section>
				<section>
					<h3>Restez pragmatique !</h3>
					<ul>
						<li class="fragment">Analyse de l'existant</li>
						<li class="fragment">Modélisation et extrapolation</li>
						<li class="fragment">Test de l'existant</li>
						<li class="fragment">Test de la cible</li>
						<li class="fragment">Test end-to-end intégré de la cible</li>
					</ul>
					<img class="fragment" height="250px" src="images/fact2/arrow.png" style="float:left; margin-left:150px;"/>
					<br/>
					<br/>
					<p class="fragment">! Certains problèmes devront passer par plusieurs étapes, afin de limiter le risque !</p>
					<p class="fragment">Ne pas sautez les étapes !</p>
				</section>
				<section>
					<h3>Où pouvez-vous vous "planter" ?</h3>
					<p>Rappel : vous ne pourrez pas garantir à 100% la performance</p>
					<div class="fragment">
						<p>Vous pouvez vous planter car :</p>
						<ul>
							<li>un risque est passé à travers le brainstorming</li>
							<li>une simplification était trop "simpliste"</li>
						</ul>
					</div>
					<div class="fragment">
						<p>D'expérience, les mauvaises décisions étaient dues à :</p>
						<ul>
							<li>une mauvaise connaissance de la performance existante</li>
							<li>une mauvaise connaissance des usages existants</li>
						</ul>
					</div>
				</section>
				<!-- Partie 3 -->
                <section data-background="images/fact3/DontForgetBasics.jpg" style="background: rgba(255,255,255,0.9)">
                    <h3>Ne pas oublier les basics</h3>
					<aside class="notes">
                        On vous a expliquer un certain nombre de façon de faire pour les tests de performance d'un SI
                        Est-ce que cela veut dire que les techniques de tests de charge traditionnelles sont inutiles?
						http://www.morguefile.com/archive/display/907874
					</aside>
                </section>
				<section>
					<p>Les tests de charges application par application restent utiles</p>
					<p>Les bonnes pratiques qui leur sont associées également</p>
					<aside class="notes">
						Deux bonnes pratiques sont particulièrement importantes pour des tests de performance d'un SI
					</aside>
				</section>
                <section>
                    <h3>Délimiter le périmètre testé</h3>
                    <p class="fragment">Car un test de charge reste un test automatisé</p>
                    <p class="fragment">Car un test en erreur c'est un test qui ne sert à rien</p>
                    <aside class="notes">Si vous ne délimitez pas correctement le périmètre testé vous allez au devant de
                    problèmes insurmontables.
                    </aside>
                </section>
                <section>
                    <h3>Comment le délimiter ?</h3>
                    <p class="fragment">Choisissez soigneusement votre jeux de données</p>
                    <p class="fragment">Ou développez des bouchons</p>
                    <aside class="notes">
                        Définissez soigneusement votre jeu de données de façon à ce que l'ensemble des systèmes sollicités
                        répondent correctement. Etudiez la charge que cela va induire dans tous les systèmes avals :
                        requête sur une seule et même resssource ? requêtes sur un jeu de données élargi ? sur un jeu
                        de données représentatif ?
                    </aside>
                </section>
                <section>
                    <h1>Mettre un exemple concret ?</h1>
                </section>
                <section>
                    <h2>Modélisation de la charge</h2>
					<p>Comment modéliser le comportement de mes utilisateurs ?</p>
					<p>Combien d'utilisateurs simultanés ?</p>
					<p>Qu'est-ce qu'un utilisateur simultané ?</p>
                    <aside class="notes">
                        Pour une application existante connaître le nombre d'utilisateurs est assez simple.
                        Mais pour un SI entier ? Pour une application qui ne reçoit des demandes que d'autres
                        applications ?
                    </aside>
                </section>
				<section>
					Modélisation de la charge : Comment modéliser le comportement de mes utilisateurs ?
					<p>La véritée est en production</p>
					<img height="238" data-src="./images/mdlcharge/stats.png" alt="Modèles statistiques">
					<p>Construisez un modèle à partir de vos statistiques de production</p>
					<aside class="notes">
						La mesure d'un temps de réponse sera toujours statistique. En effet l'arrivée des utilsiateurs sera toujours aléatoire.
						Tous les tests de charge vont donc reposer sur la mesure statistique du temps de réponse suite à une injection. 
						Le modélisation des uti
                        lisateurs est basée sur une statistique d'arrivée et une statistique d'un temps de réflexion (expliquer)
						<a href="F:\users\mbojoly\Google Drive\Pole Performance\Chantiers\OctoAcademy\Performance et scalabilite.pptx">Image</a>
					</aside>
				</section>
				<section>
					<p>Modélisation de la charge : Soyez prédictif</p>
					<p>Si vous n'avez pas de production, proposez un modèle en fonction des informations que vous avez.</p>
					<img height="238" data-src="./images/mdlcharge/adopters.png" alt="Modèle de prévision">
					<aside class="notes">
					Par exemple basez vous sur des modèles métiers : combien de nouveaux utilisateurs ?
					</aside>
				</section>
				<section>
					<p>Modélisation de la charge : Soyez prédictif</p>
					<p>Si vous n'avez pas de production, proposez un modèle en fonction des informations que vous avez.</p>
					<img height="238" data-src="./images/mdlcharge/trafficInternet.png" alt="Modèle de prévision">
					<aside class=   "notes">
					Par exemple basez vous sur des modèles métiers : quelle est l'utilisation classique d'internet ?
					<a href="http://www.libstat.com/pages/heure.htm">Libstat</a> 
					<a href="F:\Users\mbojoly\Documents\MBO\2014.zip\2014\MyDrive\R2014-168 PARKEON - Tests de perfs Plateforme billettique Lille\4 - Documents de travail\ModeleMonteeEnCharge.xlsx">Image</a>
					</aside>
				</section>
				<section>
					<p>Modélisation de la charge : comment l'utiliser ?</p>
					Définissez dans vos tests d'injection locaux le nombre d'utilisateurs "simultanés" et le temps de réflexion
					<pre><code data-trim contenteditable class="scala" >
val clientSearchChain = group("client_search_page") {
	exec(http("client_search_html")
	.get("""/ebankingAdmin/acibf/root/contract/contractlist/""")
}
).pause(7,8) //Pause between 7 and 8 seconds

val scn = scenario("AdminSimulation")
	.repeat(1) { //Beware of the coordinated omission
		exitBlockOnFail {
			exec(loginChain)
			.exec(clientSearchChain)
		}
	}

setUp(scn.inject(
    rampUsers(120).over(60), //Needs to be adapted to your scenario
).protocols(httpProtocol)
</code></pre>
					<aside class="notes">
                        Coordinated Omission, c'est quant ton injecteur réduit sa charge proportionnellement à
                        la lenteur de ton applicatif en charge. Ça arrive avec LoadRunner parce qu'il compte en
                        utilisateurs simultanés et pas en nouveaux utilisateurs par unité de temps ou actions par unité de temps.
                        <a href="F:\users\mbojoly\Google Drive\SHARED ASSETS\SHARED ASSETS - PERF\06. PERF - Advanced users\Formations\Pepperdine_20140707\performance.zip\performance\slides\04_benchmarking.pdf">
                            Slide 86 Formation Kirk</a>
					</aside>
				</section>
                <section>
                    <h3>Ne négligez pas les tests de charge unitaire</h3>
                    <p class="fragment">La vérité est toujours dans la mesure</p>
                    <p><span class="fragment">Comment avaler un éléphant ?</span>&nbsp;<span class="fragment">Bouchée par bouchée</span></p>
                    <p class="fragment"></p>
                    <aside class="notes">
                        Ne négligez pas les tests de charge unitaire. Choisissez les le plus précisément possible pour
                        qu'ils soient simples, reproductibles, compréhensibles.
                        C'est le seul moyen de réutiliser ces résultats dans des conclusions plus larges
                    </aside>
                </section>
				<!-- Partie 4 -->
				<section data-background="images/fact4/tests-end-to-end.jpg" style="background: rgba(255,255,255,0.9)">
					Comment (quand même) tester en end-to-end ?
				</section>
                <section>
                    <h3>Les problèmes faibles prennent de l'ampleur au niveau d'un SI</h3>
                    <aside class="notes">
                        Certaines problématiques qui restent gérables au niveau d'une application sont
                        catastrophiques au niveau d'un SI
                    </aside>
                </section>
                <section>
                    <h3>POC</h3>
                    <img src="images/fact4/PocSchema.png" />
                    <aside class="notes">
                        L'objectif de ce POC est de modéliser un SI pour voir l'impact de ces problèmes
                        Pour cela nous avons utilisé docker pour représenter les applications au sein d'une seule VM
                    </aside>
                </section>
                <section>
					<section>
						<h3>DEMO : une application</h3>
						<p>5 appels en base de données, 1 traitement</p>
					</section>
					<section>
					<pre><code data-trim contenteditable class="shell" >
curl -X POST \
-H "Accept: applicaiton/json" \
-H "Content-Type: application/json" \
-d '{"cpuIntensiveComputationsDuration":50, "databaseCallsNumber":5, "databaseCallDuration":10 }' \
http://192.168.99.100:8080/compute
					</code></pre>
					</section>
                </section>
                <section>
					<section>
						<h3>DEMO : on passe à l'échelle avec un petit SI</h3>
                        <p>5 applications qui s'appellent successivement de façon synchrone</p>
						<p>5 appels en base de données, 1 traitement</p>
					</section>
                    <section><pre><code data-trim contenteditable class="shell" >
curl -X POST \
-H "Accept: applicaiton/json" \
-H "Content-Type: application/json" \
-d '{"cpuIntensiveComputationsDuration":50, "databaseCallsNumber":5, "databaseCallDuration":10, \
"serviceCalls":[ \
{"computationDescription":{"cpuIntensiveComputationsDuration":50, "databaseCallsNumber":5, "databaseCallDuration":5}, "callsNumber":2 }, \
{"computationDescription":{"cpuIntensiveComputationsDuration":50, "databaseCallsNumber":5, "databaseCallDuration":5}, "callsNumber":2 }, \
{"computationDescription":{"cpuIntensiveComputationsDuration":50, "databaseCallsNumber":5, "databaseCallDuration":5}, "callsNumber":2 }, \
{"computationDescription":{"cpuIntensiveComputationsDuration":50, "databaseCallsNumber":5, "databaseCallDuration":5}, "callsNumber":2 }
] \
}' \
http://192.168.99.100:8080/compute
						</code></pre>
                    </section>
                </section>
				<section>
					<section>
						<h3>DEMO : on passe dans un SI réel</h3>
					</section>
					<section><pre><code data-trim contenteditable class="shell" >
$ sudo docker-machine ssh default
$$ sudo tc qdisc add dev docker0 root netem delay 10ms
$$ sudo tc qdisc show dev docker0
					</code></pre></section>
					<section>
						Sans latence
						<pre><code data-trim contenteditable class="shell" >
Call the database 5 times during 10 ms. each for a total of 57 ms.
CPU intensive compute 45ms.
Call HTTP Ressources : For a total of 0.0 ms.
						</code></pre>
						Avec 10 ms. de latence
						<pre><code data-trim contenteditable class="shell" >
Call the database 5 times during 10 ms. each for a total of 807 ms.
CPU intensive compute 48ms.
Call HTTP Ressources : For a total of 0.0 ms.
						</code></pre>
					</section>
				</section>
                <section>
                    <h3>Et concrêtement ? </h3>
                    <img src="images/fact4/DockerTcp80.png" />
                </section>
                <section>
                    <h3>Et concrêtement ?</h3>
                    <img src="images/fact4/DockerTcp9093.png" />
                </section>
                <section><img src="images/fact4/Peage.jpg"></section>
                <section>
                    <h3>Nous avons simulé une DMZ un peu pointilleuse</h3>
                    <img src="images/fact4/DMZ.png" />
                </section>
                <section>
                    <h3>En synthèse, les problèmes faibles peuvent devenir critiques au niveau d'un SI</h3>
                    <ul>
                        <li class="fragment">Latence</li>
                        <li class="fragment">N1 requêtes</li>
                        <li class="fragment">N+1 requêtes imbriquées</li>
                    </ul>
                </section>
				<section>
					<p>Les tests end-to-end sont obligatoires car certains résultats (cf. PoC) peuvent défier l'intuition</p>
					<div class="fragment">
						<h3 >Disclaimer</h3>
						<p>Les tests end-to-end sont complexes. Ceci ne se veut pas une méthode systémique mais un inventaire
							d'outils à disposition, ainsi qu'un REX sur leur efficacité</p>
					</div>
				</section>
				<section>
					<h3>Outil #1 : Analyse de la production existante</h3>
					<p class="fragment">Analyse des latences entre systèmes</p>
					<p class="fragment">Identification du goulet d'étranglement</p>
					<p class="fragment">Découverte des problèmes de design</p>
					<p class="fragment">Évaluation de la capacité du système</p>
				</section>
				<section>
					<h3>Outil #1 : Analyse de la production existante</h3>
					<p>Exemple de l'évaluation de la capacité du système</p>
					<img src="images/fact4/order-throughput.png" />
				</section>
				<section>
					<h3>Outil #1 : mon REX</h3>
					<p class="fragment">Souvent faîte trop tard...</p>
					<p class="fragment">Dur à vendre, pourtant ROI elevé</p>
					<p class="fragment">Commencez petit...</p>
					<p class="fragment">Attention à la répartition de la charge !</p>
				</section>
				<section>
					<h3>Outil #1 : attention aux chiffres !</h3>
					<!-- TODO add image comparison -->
				</section>
				<section>
					<h3>Outil #1 : attention aux chiffres !</h3>
					<!-- TODO add image comparison -->
				</section>
				<section>
					<h3>Outil #2 : Benchmark unitaire</h3>
					<p><em>Définition : mesure de la réponse à une transaction unitaire</em></p>
				</section>
				<section>
					<h3>Outil #2 : Benchmark unitaire</h3>
					<p>Exemple d'analyse</p>
					<img height="200px" src="images/fact4/response-time.png" />
					<div class="fragment">
						<p>Une représentation en "flame graph" est également adaptée</p>
						<img height="150px" src="images/fact4/flame-graph.png" />
					</div>
				</section>
				<section>
					<h3>Outil #2 : mon REX</h3>
					<p class="fragment">La plupart des problèmes de design apparaissent en unitaire</p>
					<p class="fragment">Faîtes-en plusieurs...</p>
					<p class="fragment">Attention à ne pas faire de test de charge avant !</p>
					<p class="fragment">S'il ne faut en garder qu'un, gardez celui-ci !</p>
				</section>
				<aside class="notes">
					Ex. problème GTX
					Parallèle avec test d'un seul système
				</aside>
				<section>
					<h3>Outil #3 : le Dirac</h3>
					<img height="400px" src="images/fact4/dirac.png" />
					<aside class="notes">
						Peak de N transactions
						ou 1 transaction sur un gros volume
					</aside>
				</section>
				<section>
					<h3>Outil #3 : le Dirac</h3>
					<p>Exemple d'analyse</p>
					<img height="400px" src="images/fact4/dirac-example.png" />
				</section>
				<section>
					<h3>Outil #3 : mon REX</h3>
					<p class="fragment">Le complément parfait du test unitaire</p>
					<p class="fragment">Plus simple à analyser qu'une journée de production</p>
					<p class="fragment">Teste également la robustesse du système</p>
					<p class="fragment">Greffez-vous sur les tests fonctionnels !</p>
				</section>
				<section>
					<h3>Outil #4 : "rejeu" d'une journée de production</h3>
					<img height="500px" src="images/fact4/replay-production.png" />
					<p style="position: absolute; top:450px; left: 200px; height:50px; width:600px; font-size:1.5em; color:blue;" class="fragment">simplifiée bien sûr !</p>
				</section>
				<section>
					<h3>Outil #4 : mon REX</h3>
					<p class="fragment">Difficile à analyser, notamment les écarts</p>
					<p class="fragment">Difficile à interpréter</p>
					<p class="fragment">En dernier, en impliquant le support métier</p>
				</section>
				<section>
					<h3>Outil #1 et #4 : retour aux bases</h3>
					<p class="fragment">La décomposition en unitaire et Dirac simplifie grandement l'analyse</p>
				</section>
				<section>
					<h3>Outil #5 : mettre en production</h3>
					<p class="fragment">Tout ce qui peut être mis en production par avance doit l'être</p>
					<p class="fragment">Pensez votre stratégie de migration pour monter en charge progressivement</p>
					<p class="fragment">Inspirez vous des Géants du Web (feature flipping, dark launch, canary)</p>
				</section>
				<section>
					<h3>Que faut-il mesurer ?</h3>
					<!-- TODO -->
					<p class="fragment">Temps entrée / sortie de chaque système</p>
					<p class="fragment">NMON (incl. TOP)</p>
					<p class="fragment">à complèter si nécessaire...</p>
				</section>
				<section>
					<h3>Que faut-il analyser ?</h3>
					<!-- TODO -->
					<img />
					<p>Throughput, temps de réponse/charge</p>
					<aside class="notes">
						Analyse auto
						Sur HW juste sanity check (TOP, CPU, RAM)
					</aside>
				</section>

				<section>
					<h3>Peut-on extrapoler (à la production) ?</h3>
					<p class="fragment">Beaucoup d'extrapolations différentes possibles</p>
					<p class="fragment">Environnement, concurrence, simplifications...</p>
					<p class="fragment">Mon expérience : OUI (sauf le sizing machine)</p>
					<p class="fragment">mais attention de rester "linéaire" dans vos tests !</p>
				</section>
				<section>
					<h3>Où pouvez-vous vous "planter" ?</h3>
					<p>Rappel : vous ne pourrez pas garantir à 100% la performance</p>
					<ul>
						<li class="fragment">si vous ne faîtes pas assez d'unitaire</li>
						<li class="fragment">si votre approche n'est pas assez "data-driven"</li>
						<li class="fragment">si vous n'avez pas assez simplifié / découplé</li>
					</ul>
				</section>
				<!-- Conclusion -->
				<section>
					<h3>Conclusion</h3>
					<ul>
						<li>Commencez petit</li>
						<li>Mesurez</li>
						<li>Généralisez : oui mais....</li>
					</ul>
				</section>
                <section data-background="images/context/DSCN7614.jpg" style="background: rgba(255,255,255,0.9)">
                    Reste à attendre la mise en production...
                    <aside class="notes">
                        Désormais vous êtes beaucoup mieux outillés pour gérer les problèmes de performance
                        en production
                    </aside>
                </section>


			</div>

		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>

			// Full list of configuration options available at:
			// https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({
				controls: true,
				progress: true,
				history: true,
				center: true,
				slideNumber: true,

				transition: 'slide', // none/fade/slide/convex/concave/zoom

				// Optional reveal.js plugins
				dependencies: [
					{ src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
					{ src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/highlight/highlight.js', async: true, condition: function() { return !!document.querySelector( 'pre code' ); }, callback: function() { hljs.initHighlightingOnLoad(); } },
					{ src: 'plugin/zoom-js/zoom.js', async: true },
					{ src: 'plugin/notes/notes.js', async: true }
				]
			});

		</script>

	</body>
</html>
